<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Yunze's blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="LinearRegression"><meta name=generator content="Hugo 0.101.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content><meta property="og:description" content="LinearRegression"><meta property="og:type" content="article"><meta property="og:url" content="https://arthuryunze.github.io/posts/2020-02-28-machinelearning%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><meta property="article:section" content="posts"><meta itemprop=name content><meta itemprop=description content="LinearRegression"><meta itemprop=wordCount content="48"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content><meta name=twitter:description content="LinearRegression"></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Yunze's blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1"></h1></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id=线性回归>线性回归</h2><h1 id=线性模型>线性模型</h1><h2 id=基本形式>基本形式</h2><p>线性模型(Linear model)试图学得一个通过属性的线性组合来进行预测的函数，即</p><p>$$ f(x)=w_1x_1+w_2x_2+&mldr;+w_dx_d+b $$</p><p>一般用向量形式写成</p><p>$$ f(x)=w^Tx+b $$</p><p>其中$ w=(w_1;w_2;&mldr;;w_d) $.w和b学得之后，模型就得以确定。</p><p>线性模型有很好的可解释性。</p><h2 id=线性回归-1>线性回归</h2><p>&ldquo;线性回归&rdquo; (linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记。</p><p>均方误差(2.2) 是回归任务中最常用的性能度量</p><p>均方误差有非常好的几何意义。它对应了常用的欧几里得距离或简称"欧氏距离" (Euclidean distance). 基于均方误差最小化来进行模型求解的方法称为"最小二乘法" (least squ町e method).</p><p>在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小.</p><p>求解w,b过程，称为线性回归模型的最小二乘"参数估计" (parameter estimation).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python></code></pre></div><p>凸函数的任何极小值也是最小值。严格凸函数最多有一个最小值。
<a href=https://zh.wikipedia.org/zh-hans/%E5%87%B8%E5%87%BD%E6%95%B0>凸函数 - 维基百科，自由的百科全书</a></p><hr><p>两种算法：</p><h2 id=经典算法最小二乘法>经典算法——最小二乘法</h2><h2 id=深度学习梯度下降法>深度学习——梯度下降法</h2><p>用两种方法分别实现了线性回归
项目传到GitHub
<a href=https://github.com/arthuryunze/Learn_MXNet/blob/master/ML%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A0/LinearRegression.ipynb>Learn_MXNet/LinearRegression.ipynb at master · arthuryunze/Learn_MXNet</a>
<a href=https://blog.csdn.net/sxf1061926959/article/details/66976356>线性回归理解（附纯python实现）_Python_快来学习鸭～～～-CSDN博客</a></p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://arthuryunze.github.io/>&copy; Yunze's blog 2022</a><div><div class=ananke-socials></div></div></div></footer></body></html>
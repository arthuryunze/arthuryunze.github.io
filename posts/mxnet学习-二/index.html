<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>MXNet学习-二 | Yunze's blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="description: Learn MXNet 2 categories: ML MXNet 在上一节，我们已经搭好了LeNet。
现在，我们要开始使用数据训练这个网络，以使它找到合适的参数。
我们将使用超参数训练LeNet。
若使用GPU进行运算，我们只需要将mx.cpu()更改为mx.gpu()，而MXNet会处理其余的工作。我们将在10个时间段(epoch)后停止训练。
建议使用GPU进行运算，LeNet比MLP(多层感知机)更复杂，计算量更大。GPU可大大加快计算速度。
为了方便，我的环境使用了CPU版本的MXNet。速度会稍慢，但不影响后续代码。
注意CPU版本和GPU版本安装时是不同的，安装步骤可以查看MXNet官网。
Initialize parameters and optimizer——初始化参数和优化器 初始化网络参数如下：
# set the context on GPU is available otherwise CPU ctx = [mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()] net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx) trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03}) Training——训练 # Use Accuracy as the evaluation metric. metric = mx.metric.Accuracy() softmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss() for i in range(epoch): # Reset the train data iterator. train_data.reset() # Loop over the train data iterator."><meta name=generator content="Hugo 0.101.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content="MXNet学习-二"><meta property="og:description" content="description: Learn MXNet 2 categories: ML MXNet 在上一节，我们已经搭好了LeNet。
现在，我们要开始使用数据训练这个网络，以使它找到合适的参数。
我们将使用超参数训练LeNet。
若使用GPU进行运算，我们只需要将mx.cpu()更改为mx.gpu()，而MXNet会处理其余的工作。我们将在10个时间段(epoch)后停止训练。
建议使用GPU进行运算，LeNet比MLP(多层感知机)更复杂，计算量更大。GPU可大大加快计算速度。
为了方便，我的环境使用了CPU版本的MXNet。速度会稍慢，但不影响后续代码。
注意CPU版本和GPU版本安装时是不同的，安装步骤可以查看MXNet官网。
Initialize parameters and optimizer——初始化参数和优化器 初始化网络参数如下：
# set the context on GPU is available otherwise CPU ctx = [mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()] net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx) trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03}) Training——训练 # Use Accuracy as the evaluation metric. metric = mx.metric.Accuracy() softmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss() for i in range(epoch): # Reset the train data iterator. train_data.reset() # Loop over the train data iterator."><meta property="og:type" content="article"><meta property="og:url" content="https://arthuryunze.github.io/posts/mxnet%E5%AD%A6%E4%B9%A0-%E4%BA%8C/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-02-21T16:57:58+08:00"><meta property="article:modified_time" content="2020-02-21T16:57:58+08:00"><meta itemprop=name content="MXNet学习-二"><meta itemprop=description content="description: Learn MXNet 2 categories: ML MXNet 在上一节，我们已经搭好了LeNet。
现在，我们要开始使用数据训练这个网络，以使它找到合适的参数。
我们将使用超参数训练LeNet。
若使用GPU进行运算，我们只需要将mx.cpu()更改为mx.gpu()，而MXNet会处理其余的工作。我们将在10个时间段(epoch)后停止训练。
建议使用GPU进行运算，LeNet比MLP(多层感知机)更复杂，计算量更大。GPU可大大加快计算速度。
为了方便，我的环境使用了CPU版本的MXNet。速度会稍慢，但不影响后续代码。
注意CPU版本和GPU版本安装时是不同的，安装步骤可以查看MXNet官网。
Initialize parameters and optimizer——初始化参数和优化器 初始化网络参数如下：
# set the context on GPU is available otherwise CPU ctx = [mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()] net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx) trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03}) Training——训练 # Use Accuracy as the evaluation metric. metric = mx.metric.Accuracy() softmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss() for i in range(epoch): # Reset the train data iterator. train_data.reset() # Loop over the train data iterator."><meta itemprop=datePublished content="2020-02-21T16:57:58+08:00"><meta itemprop=dateModified content="2020-02-21T16:57:58+08:00"><meta itemprop=wordCount content="318"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="MXNet学习-二"><meta name=twitter:description content="description: Learn MXNet 2 categories: ML MXNet 在上一节，我们已经搭好了LeNet。
现在，我们要开始使用数据训练这个网络，以使它找到合适的参数。
我们将使用超参数训练LeNet。
若使用GPU进行运算，我们只需要将mx.cpu()更改为mx.gpu()，而MXNet会处理其余的工作。我们将在10个时间段(epoch)后停止训练。
建议使用GPU进行运算，LeNet比MLP(多层感知机)更复杂，计算量更大。GPU可大大加快计算速度。
为了方便，我的环境使用了CPU版本的MXNet。速度会稍慢，但不影响后续代码。
注意CPU版本和GPU版本安装时是不同的，安装步骤可以查看MXNet官网。
Initialize parameters and optimizer——初始化参数和优化器 初始化网络参数如下：
# set the context on GPU is available otherwise CPU ctx = [mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()] net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx) trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03}) Training——训练 # Use Accuracy as the evaluation metric. metric = mx.metric.Accuracy() softmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss() for i in range(epoch): # Reset the train data iterator. train_data.reset() # Loop over the train data iterator."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Yunze's blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">MXNet学习-二</h1><time class="f6 mv4 dib tracked" datetime=2020-02-21T16:57:58+08:00>February 21, 2020</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><hr><h2 id=categories-ml-mxnet>description: Learn MXNet 2
categories: ML MXNet</h2><p>在上一节，我们已经搭好了LeNet。</p><p>现在，我们要开始使用数据训练这个网络，以使它找到合适的参数。</p><p>我们将使用超参数训练LeNet。</p><p>若使用GPU进行运算，我们只需要将mx.cpu()更改为mx.gpu()，而MXNet会处理其余的工作。我们将在10个时间段(epoch)后停止训练。</p><blockquote><p>建议使用GPU进行运算，LeNet比MLP(多层感知机)更复杂，计算量更大。GPU可大大加快计算速度。</p></blockquote><blockquote><p>为了方便，我的环境使用了CPU版本的MXNet。速度会稍慢，但不影响后续代码。</p><blockquote><p>注意CPU版本和GPU版本安装时是不同的，安装步骤可以查看MXNet官网。</p></blockquote></blockquote><h3 id=initialize-parameters-and-optimizer初始化参数和优化器>Initialize parameters and optimizer——初始化参数和优化器</h3><p>初始化网络参数如下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># set the context on GPU is available otherwise CPU</span>
</span></span><span style=display:flex><span>ctx <span style=color:#f92672>=</span> [mx<span style=color:#f92672>.</span>gpu() <span style=color:#66d9ef>if</span> mx<span style=color:#f92672>.</span>test_utils<span style=color:#f92672>.</span>list_gpus() <span style=color:#66d9ef>else</span> mx<span style=color:#f92672>.</span>cpu()]
</span></span><span style=display:flex><span>net<span style=color:#f92672>.</span>initialize(mx<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>Xavier(magnitude<span style=color:#f92672>=</span><span style=color:#ae81ff>2.24</span>), ctx<span style=color:#f92672>=</span>ctx)
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> gluon<span style=color:#f92672>.</span>Trainer(net<span style=color:#f92672>.</span>collect_params(), <span style=color:#e6db74>&#39;sgd&#39;</span>, {<span style=color:#e6db74>&#39;learning_rate&#39;</span>: <span style=color:#ae81ff>0.03</span>})
</span></span></code></pre></div><h3 id=training训练>Training——训练</h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Use Accuracy as the evaluation metric.</span>
</span></span><span style=display:flex><span>metric <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>metric<span style=color:#f92672>.</span>Accuracy()
</span></span><span style=display:flex><span>softmax_cross_entropy_loss <span style=color:#f92672>=</span> gluon<span style=color:#f92672>.</span>loss<span style=color:#f92672>.</span>SoftmaxCrossEntropyLoss()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(epoch):
</span></span><span style=display:flex><span>    <span style=color:#75715e># Reset the train data iterator.</span>
</span></span><span style=display:flex><span>    train_data<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>    <span style=color:#75715e># Loop over the train data iterator.</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> train_data:
</span></span><span style=display:flex><span>        <span style=color:#75715e># Splits train data into multiple slices along batch_axis</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># and copy each slice into a context.</span>
</span></span><span style=display:flex><span>        data <span style=color:#f92672>=</span> gluon<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>split_and_load(batch<span style=color:#f92672>.</span>data[<span style=color:#ae81ff>0</span>], ctx_list<span style=color:#f92672>=</span>ctx, batch_axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Splits train labels into multiple slices along batch_axis</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># and copy each slice into a context.</span>
</span></span><span style=display:flex><span>        label <span style=color:#f92672>=</span> gluon<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>split_and_load(batch<span style=color:#f92672>.</span>label[<span style=color:#ae81ff>0</span>], ctx_list<span style=color:#f92672>=</span>ctx, batch_axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>        outputs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#75715e># Inside training scope</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> ag<span style=color:#f92672>.</span>record():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> x, y <span style=color:#f92672>in</span> zip(data, label):
</span></span><span style=display:flex><span>                z <span style=color:#f92672>=</span> net(x)
</span></span><span style=display:flex><span>                <span style=color:#75715e># Computes softmax cross entropy loss.</span>
</span></span><span style=display:flex><span>                loss <span style=color:#f92672>=</span> softmax_cross_entropy_loss(z, y)
</span></span><span style=display:flex><span>                <span style=color:#75715e># Backpropogate the error for one iteration.</span>
</span></span><span style=display:flex><span>                loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>                outputs<span style=color:#f92672>.</span>append(z)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Updates internal evaluation</span>
</span></span><span style=display:flex><span>        metric<span style=color:#f92672>.</span>update(label, outputs)
</span></span><span style=display:flex><span>        <span style=color:#75715e># Make one step of parameter update. Trainer needs to know the</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># batch size of data to normalize the gradient by 1/batch_size.</span>
</span></span><span style=display:flex><span>        trainer<span style=color:#f92672>.</span>step(batch<span style=color:#f92672>.</span>data[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>    <span style=color:#75715e># Gets the evaluation result.</span>
</span></span><span style=display:flex><span>    name, acc <span style=color:#f92672>=</span> metric<span style=color:#f92672>.</span>get()
</span></span><span style=display:flex><span>    <span style=color:#75715e># Reset evaluation result to initial state.</span>
</span></span><span style=display:flex><span>    metric<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>&#39;training acc at epoch </span><span style=color:#e6db74>%d</span><span style=color:#e6db74>: </span><span style=color:#e6db74>%s</span><span style=color:#e6db74>=</span><span style=color:#e6db74>%f</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>%</span>(i, name, acc))
</span></span></code></pre></div><h3 id=prediction预测>Prediction——预测</h3><p>最后，我们将使用训练好的LeNet模型来生成对测试数据的预测。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%%</span>time
</span></span><span style=display:flex><span>epoch <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>metric <span style=color:#f92672>=</span> mx<span style=color:#f92672>.</span>metric<span style=color:#f92672>.</span>Accuracy()
</span></span><span style=display:flex><span><span style=color:#75715e># Reset the validation data iterator.</span>
</span></span><span style=display:flex><span>val_data<span style=color:#f92672>.</span>reset()
</span></span><span style=display:flex><span><span style=color:#75715e># Loop over the validation data iterator.</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> batch <span style=color:#f92672>in</span> val_data:
</span></span><span style=display:flex><span>    <span style=color:#75715e># Splits validation data into multiple slices along batch_axis</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># and copy each slice into a context.</span>
</span></span><span style=display:flex><span>    data <span style=color:#f92672>=</span> gluon<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>split_and_load(batch<span style=color:#f92672>.</span>data[<span style=color:#ae81ff>0</span>], ctx_list<span style=color:#f92672>=</span>ctx, batch_axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># Splits validation label into multiple slices along batch_axis</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># and copy each slice into a context.</span>
</span></span><span style=display:flex><span>    label <span style=color:#f92672>=</span> gluon<span style=color:#f92672>.</span>utils<span style=color:#f92672>.</span>split_and_load(batch<span style=color:#f92672>.</span>label[<span style=color:#ae81ff>0</span>], ctx_list<span style=color:#f92672>=</span>ctx, batch_axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    outputs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> x <span style=color:#f92672>in</span> data:
</span></span><span style=display:flex><span>        outputs<span style=color:#f92672>.</span>append(net(x))
</span></span><span style=display:flex><span>    <span style=color:#75715e># Updates internal evaluation</span>
</span></span><span style=display:flex><span>    metric<span style=color:#f92672>.</span>update(label, outputs)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;validation acc: </span><span style=color:#e6db74>%s</span><span style=color:#e6db74>=</span><span style=color:#e6db74>%f</span><span style=color:#e6db74>&#39;</span><span style=color:#f92672>%</span>metric<span style=color:#f92672>.</span>get())
</span></span><span style=display:flex><span><span style=color:#66d9ef>assert</span> metric<span style=color:#f92672>.</span>get()[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.98</span>
</span></span></code></pre></div><p>如果一切顺利，我们将看到使用LeNet进行预测的更高精度度量。<br>有了CNN，我们应该能够正确预测98%的测试图像。</p><h3 id=总结summary>总结——Summary</h3><p>在本教程中，我们学习了如何使用MXNet解决一个标准的计算机视觉问题:对手写数字的图像进行分类。您已经了解了如何使用MXNet胶子包快速、轻松地构建、培训和评估MLP和CNN等模型。</p><p>本实验已上传至Github。<br><a href=https://github.com/arthuryunze/Learn_MXNet/blob/master/mnist-mxnet.ipynb>Learn_MXNet/mnist-mxnet.ipynb at master · arthuryunze/Learn_MXNet</a></p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://arthuryunze.github.io/>&copy; Yunze's blog 2022</a><div><div class=ananke-socials></div></div></div></footer></body></html>
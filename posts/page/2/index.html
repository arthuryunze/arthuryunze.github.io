<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Yunze's blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content><meta name=generator content="Hugo 0.101.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><link href=/posts/index.xml rel=alternate type=application/rss+xml title="Yunze's blog"><link href=/posts/index.xml rel=feed type=application/rss+xml title="Yunze's blog"><meta property="og:title" content="Posts"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://arthuryunze.github.io/posts/"><meta itemprop=name content="Posts"><meta itemprop=description content><meta name=twitter:card content="summary"><meta name=twitter:title content="Posts"><meta name=twitter:description content></head><body class="ma0 avenir bg-near-white"><header><div class="pb3-m pb6-l bg-black"><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Yunze's blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav><div class="tc-l pv3 ph3 ph4-ns"><h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">Posts</h1></div></div></header><main class=pb7 role=main><article class="pa3 pa4-ns nested-copy-line-height"><section class="cf ph3 ph5-l pv3 pv4-l f4 tc-l center measure-wide lh-copy mid-gray"></section><section class="flex-ns flex-wrap justify-around mt5"><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=/posts/2020-02-21-mxnet%E5%AD%A6%E4%B9%A0-%E4%BA%8C/ class="link black dim"></a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">在上一节，我们已经搭好了LeNet。
现在，我们要开始使用数据训练这个网络，以使它找到合适的参数。
我们将使用超参数训练LeNet。
若使用GPU进行运算，我们只需要将mx.cpu()更改为mx.gpu()，而MXNet会处理其余的工作。我们将在10个时间段(epoch)后停止训练。
建议使用GPU进行运算，LeNet比MLP(多层感知机)更复杂，计算量更大。GPU可大大加快计算速度。
为了方便，我的环境使用了CPU版本的MXNet。速度会稍慢，但不影响后续代码。
注意CPU版本和GPU版本安装时是不同的，安装步骤可以查看MXNet官网。
Initialize parameters and optimizer——初始化参数和优化器 初始化网络参数如下：
# set the context on GPU is available otherwise CPU ctx = [mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()] net.initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx) trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.03}) Training——训练 # Use Accuracy as the evaluation metric. metric = mx.metric.Accuracy() softmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss() for i in range(epoch): # Reset the train data iterator. train_data.reset() # Loop over the train data iterator. for batch in train_data: # Splits train data into multiple slices along batch_axis # and copy each slice into a context.</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=/posts/2020-02-23-face-detectionmtcnn-%E5%AD%A6%E4%B9%A0-%E4%B8%80/ class="link black dim"></a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Face Detection 人脸检测 对于人类而言，这是一个微不足道的问题，近来也已经被经典的基于特征的技术（例如级联分类器）合理地解决了。最近，深度学习方法已在标准的基准人脸检测数据集上取得了最先进的结果。一个例子是多任务级联卷积神经网络(Multi-task Cascade Convolutional Neural Network)，简称MTCNN。
MTCNN网络结构 该网络使用具有三个网络的级联结构。 首先将图像缩放到不同大小的范围（称为图像金字塔），然后第一个模型（建议网络或P-Net）提出候选的面部区域，第二个模型（Refine网络或R-Net）过滤边界框 ，第三个模型（输出网络或O-Net）提出了人脸标志。
第一个网络是一个浅层神经网络（后简称pnet） => 产生初步候选框 第二个网络是一个“more complex CNN“相较pnet更为复杂的网络（后简称rnet）=> 从初步候选框中筛选掉大量的不包括人脸的框
第三个网络是一个“more powerful CNN”相较rnet更为强大的网络（后简称onet）=> 进一步优化结果以及产生五个面部特征点（双眼，鼻尖，嘴角）
mtcnn库 GitHub上发现，已经有同学做好了MTCNN相应的库
ipazc/mtcnn: MTCNN face detection implementation for TensorFlow, as a PIP package.
我们先简单使用一下：
用笔记本自带摄像头拍摄一张照片，用opencv库导入。
from mtcnn import MTCNN import cv2 img = cv2.cvtColor(cv2.imread("WIN_20200223_17_44_49_Pro.jpg"), cv2.COLOR_BGR2RGB) detector = MTCNN() detector.detect_faces(img) 我们可以得到一个list格式的数据
[{'box': [480, 214, 234, 309], 'confidence': 0.9999535083770752, 'keypoints': {'left_eye': (536, 330), 'right_eye': (643, 334), 'nose': (583, 402), 'mouth_left': (543, 453), 'mouth_right': (633, 456)}}] 从这个list可以看出</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=/posts/2020-02-24-loss-function/ class="link black dim"></a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">What’s a Loss Function? 什么是损失函数？
At its core, a loss function is incredibly simple: it’s a method of evaluating how well your algorithm models your dataset.
损失函数的核心非常简单：它是评价你的算法效果有多好的方法。
Different Types and Flavors of Loss Functions A lot of the loss functions that you see implemented in machine learning can get complex and confusing. Consider this paper from late 2017, entitled A Semantic Loss Function for Deep Learning with Symbolic Knowledge. There’s more in that title that I don’t understand than I do.</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=/posts/2020-02-28-machinelearning%E5%9F%BA%E7%A1%80-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/ class="link black dim"></a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">线性回归 线性模型 基本形式 线性模型(Linear model)试图学得一个通过属性的线性组合来进行预测的函数，即
$$ f(x)=w_1x_1+w_2x_2+&mldr;+w_dx_d+b $$
一般用向量形式写成
$$ f(x)=w^Tx+b $$
其中$ w=(w_1;w_2;&mldr;;w_d) $.w和b学得之后，模型就得以确定。
线性模型有很好的可解释性。
线性回归 &ldquo;线性回归&rdquo; (linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记。
均方误差(2.2) 是回归任务中最常用的性能度量
均方误差有非常好的几何意义。它对应了常用的欧几里得距离或简称"欧氏距离" (Euclidean distance). 基于均方误差最小化来进行模型求解的方法称为"最小二乘法" (least squ町e method).
在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小.
求解w,b过程，称为线性回归模型的最小二乘"参数估计" (parameter estimation).
凸函数的任何极小值也是最小值。严格凸函数最多有一个最小值。 凸函数 - 维基百科，自由的百科全书
两种算法：
经典算法——最小二乘法 深度学习——梯度下降法 用两种方法分别实现了线性回归 项目传到GitHub Learn_MXNet/LinearRegression.ipynb at master · arthuryunze/Learn_MXNet 线性回归理解（附纯python实现）_Python_快来学习鸭～～～-CSDN博客</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=/posts/2020-03-01-haskell-learn-%E4%B8%80/ class="link black dim"></a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">Types and Typeclasses ghci 交互式shell中，:t判断变量类型。
:t 'a' 'a' :: Char :: is read as &ldquo;has type of&rdquo;.
函数类型显式声明 一般来说，Haskell有类型推断系统，不需要我们显式声明。但是，我们定义函数时会使用显式声明的办法。
除了编写非常短的函数之外，这通常被认为是一个好的实践。
This is generally considered to be good practice except when writing very short functions.
addThree :: Int -> Int -> Int -> Int addThree x y z = x + y + z 前三个Int是参数类型，最后一个是输出类型。 参数之间用->分隔，并且参数和返回类型之间没有特殊区别。
product [1..10] 120 1到10的乘积
head takes a list of any type and returns the first element</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=/posts/2020-03-05-%E6%9F%A5%E7%9C%8Blinux%E7%A3%81%E7%9B%98%E5%8F%8A%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8/ class="link black dim"></a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">查看Linux磁盘及内存占用 查看磁盘占用 df -k：以KB为单位显示磁盘使用量和占用率
df -m：以Mb为单位显示磁盘使用量和占用率
查看内存占用 1.top
PID：当前运行进程的ID USER：进程属主 PR：每个进程的优先级别 NInice：反应一个进程“优先级”状态的值，其取值范围是-20至19，一 共40个级别。这个值越小，表示进程”优先级”越高，而值越 大“优先级”越低。一般会把nice值叫做静态优先级 VIRT：进程占用的虚拟内存 RES：进程占用的物理内存 SHR：进程使用的共享内存 S：进程的状态。S表示休眠，R表示正在运行，Z表示僵死状态，N表示 该进程优先值为负数 %CPU：进程占用CPU的使用率 %MEM：进程使用的物理内存和总内存的百分比 TIME+：该进程启动后占用的总的CPU时间，即占用CPU使用时间的累加值。 COMMAND：进程启动命令名称
２.free
total : 总计物理内存的大小。 used : 已使用多大。 free : 可用有多少。 Shared : 多个进程共享的内存总额。 Buffers/cached : 磁盘缓存的大小。 -/+ buffers/cached) : used:已使用多大; free:可用有多少。
4.ps aux –sort -rss ps aux: 列出目前所有的正在内存当中的程序。</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=/posts/2020-03-15-socket%E7%BC%96%E7%A8%8B-%E4%B8%80/ class="link black dim"></a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">什么是socket? 在计算机通信领域，socket 被翻译为“套接字”，它是计算机之间进行通信的一种约定或一种方式。通过 socket 这种约定，一台计算机可以接收其他计算机的数据，也可以向其他计算机发送数据。
socket 的典型应用就是 Web 服务器和浏览器：浏览器获取用户输入的URL，向服务器发起请求，服务器分析接收到的URL，将对应的网页内容返回给浏览器，浏览器再经过解析和渲染，就将文字、图片、视频等元素呈现给用户。
学习 socket，也就是学习计算机之间如何通信，并编写出实用的程序。
什么是通信? IP地址和端口能够在广袤的互联网中定位到要通信的程序，协议和数据传输方式规定了如何传输数据，有了这些，两台计算机就可以通信了。
IP地址（IP Address） 计算机分布在世界各地，要想和它们通信，必须要知道确切的位置。确定计算机位置的方式有多种，IP 地址是最常用的，例如，114.114.114.114 是国内第一个、全球第三个开放的 DNS 服务地址，127.0.0.1 是本机地址。
其实，我们的计算机并不知道 IP 地址对应的地理位置，当要通信时，只是将 IP 地址封装到要发送的数据包中，交给路由器去处理。路由器有非常智能和高效的算法，很快就会找到目标计算机，并将数据包传递给它，完成一次单向通信。
端口（Port） 有了 IP 地址，虽然可以找到目标计算机，但仍然不能进行通信。一台计算机可以同时提供多种网络服务，例如Web服务、FTP服务（文件传输服务）、SMTP服务（邮箱服务）等，仅有 IP 地址，计算机虽然可以正确接收到数据包，但是却不知道要将数据包交给哪个网络程序来处理，所以通信失败。
为了区分不同的网络程序，计算机会为每个网络程序分配一个独一无二的端口号（Port Number），例如，Web服务的端口号是 80，FTP 服务的端口号是 21，SMTP 服务的端口号是 25。
协议（Protocol） 协议（Protocol）就是网络通信的约定，通信的双方必须都遵守才能正常收发数据。协议有很多种，例如 TCP、UDP、IP 等，通信的双方必须使用同一协议才能通信。协议是一种规范，由计算机组织制定，规定了很多细节，例如，如何建立连接，如何相互识别等。
协议仅仅是一种规范，必须由计算机软件来实现。例如 IP 协议规定了如何找到目标计算机，那么各个开发商在开发自己的软件时就必须遵守该协议，不能另起炉灶。
所谓协议族（Protocol Family），就是一组协议（多个协议）的统称。最常用的是 TCP/IP 协议族，它包含了 TCP、IP、UDP、Telnet、FTP、SMTP 等上百个互为关联的协议，由于 TCP、IP 是两种常用的底层协议，所以把它们统称为 TCP/IP 协议族。
数据传输方式 计算机之间有很多数据传输方式，各有优缺点，常用的有两种：SOCK_STREAM 和 SOCK_DGRAM。
SOCK_STREAM(数据流套接字) 表示面向连接的数据传输方式。数据可以准确无误地到达另一台计算机，如果损坏或丢失，可以重新发送，但效率相对较慢。常见的 http 协议就使用 SOCK_STREAM 传输数据，因为要确保数据的正确性，否则网页不能正常解析。
SOCK_DGRAM(数据报套接字) 表示无连接的数据传输方式。计算机只管传输数据，不作数据校验，如果数据在传输中损坏，或者没有到达另一台计算机，是没有办法补救的。也就是说，数据错了就错了，无法重传。因为 SOCK_DGRAM 所做的校验工作少，所以效率比 SOCK_STREAM 高。</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=/posts/2020-05-16-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%B8%89%E7%A7%8D%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AE%AD%E7%BB%83%E9%9B%86%E9%AA%8C%E8%AF%81%E9%9B%86%E6%B5%8B%E8%AF%95%E9%9B%86/ class="link black dim"></a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">机器学习中的数据集 训练集、验证集、测试集 在人工智能技术中，通常有三种常用的数据集：训练集、验证集、测试集。
先用一个不恰当的比喻来说明3种数据集之间的关系：
训练集相当于上课学知识 验证集相当于课后的的练习题，用来纠正和强化学到的知识 测试集相当于期末考试，用来最终评估学习效果 训练集 训练集（Training Dataset）是用来训练模型使用的。
机器学习的七个步骤 验证集 当我们的模型训练好之后，我们并不知道他的表现如何。
这个时候就可以使用验证集（Validation Dataset）来看看模型在新数据（验证集和测试集是不同的数据）上的表现如何。同时通过调整超参数，让模型处于最好的状态。
验证集有2个主要的作用：
评估模型效果，为了调整超参数而服务 调整超参数，使得模型在验证集上的效果最好 说明：
验证集不像训练集和测试集，它是非必需的。如果不需要调整超参数，就可以不使用验证集，直接用测试集来评估效果。 验证集评估出来的效果并非模型的最终效果，主要是用来调整超参数的，模型最终效果以测试集的评估结果为准。 测试集 当我们调好超参数后，就要开始「最终考试」了。我们通过测试集（Test Dataset）来做最终的评估。
通过测试集的评估，我们会得到一些最终的评估指标，例如：准确率、精确率、召回率、F1等。
如何合理划分数据集 下面的数据集划分方式主要针对「留出法」的验证方式，除此之外还有其他的交叉验证法，详情见下文 — — 交叉验证法。
数据划分的方法并没有明确的规定，不过可以参考3个原则：
对于小规模样本集（几万量级），常用的分配比例是 60% 训练集、20% 验证集、20% 测试集。 对于大规模样本集（百万级以上），只要验证集和测试集的数量足够即可，例如有 100w 条数据，那么留 1w 验证集，1w 测试集即可。1000w 的数据，同样留 1w 验证集和 1w 测试集。 超参数越少，或者超参数很容易调整，那么可以减少验证集的比例，更多的分配给训练集。 交叉验证法 为什么要用交叉验证法？
假如我们教小朋友学加法：1个苹果+1个苹果=2个苹果
当我们再测试的时候，会问：1个香蕉+1个香蕉=几个香蕉？
如果小朋友知道「2个香蕉」，并且换成其他东西也没有问题，那么我们认为小朋友学习会了「1+1=2」这个知识点。
如果小朋友只知道「1个苹果+1个苹果=2个苹果」，但是换成其他东西就不会了，那么我们就不能说小朋友学会了「1+1=2」这个知识点。
评估模型是否学会了「某项技能」时，也需要用新的数据来评估，而不是用训练集里的数据来评估。这种「训练集」和「测试集」完全不同的验证方法就是交叉验证法。
3 种主流的交叉验证法 留出法（Holdout cross validation） 上文提到的，按照固定比例将数据集静态的划分为训练集、验证集、测试集。的方式就是留出法。
留一法（Leave one out cross validation） 每次的测试集都只有一个样本，要进行 m 次训练和预测。 这个方法用于训练的数据只比整体数据集少了一个样本，因此最接近原始样本的分布。但是训练复杂度增加了，因为模型的数量与原始数据样本数量相同。 一般在数据缺乏时使用。</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=/posts/2020-05-28-ubuntu+apache2+mysql8%E7%8E%AF%E5%A2%83php%E5%8D%87%E7%BA%A7%E7%89%88%E6%9C%AC/ class="link black dim"></a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">这篇文章旨在记录Ubuntu+apache2+MySQL8环境下升级PHP版本（无缝升级）的经过。
原因 因某次手贱升级了MySQL数据库（版本从5到8），导致MySQL与php的兼容出了问题。
Ubuntu阿里源安装的PHP版本是7.0.3的。这个版本的PHP是不支持MySQL8新的加密方式的（caching_sha2_password），所以PHP项目在连接数据库时就会出现连接失败的问题。phpmyadmin也无法登录。
起初以为是root的远程访问权限出了问题，因为我的root权限是只限本机的（localhost），后来通过创建新的具有远程访问权限的用户发现还是无法连接，知道了跟用户权限没关系。
（Linux再也不随便升级东西了，有些软件升级后，废弃的特性和新的特性会影响各种配置）。
解决方案 找到解决方案前尝试了
修改my.cnf，发现没有这个文件，且添加文件并修改也没有效果 修改账户密码加密方式，修改后无效果 添加PHP源，这个时候PHP版本最新是7.4版本。
add-apt-repository ppa:ondrej/php
sudo apt update
sudo apt upgrade php
sudo apt install php-mysql 安装php mysql扩展。
至此，查看phpinfo界面，发现版本还是7.0，说明服务器没有使用新的版本。
查看apache2相关配置文件后，发现php是以插件的形式存在于apache2服务器中的（在/etc apache2 中什么mods enable路径下可以找到），可以使用命令打开关闭。
关闭php7.0，开启php7.4。
sudo a2dismod php7.0
sudo a2enmod php7.4
重启apache2服务器
sudo service apache2 restart
至此，服务器上的php升级完成。phpinfo页的版本也已经更新。( •̀ ω •́ )y
Reference：
(11条消息)phpmyadmin连接MySQL8.0报错#2054 - The server requested authentication method unknown to the client_数据库_weixin_40208575的博客-CSDN博客
SHA2密码验证引起的PHP错误：SQLSTATE[HY000] [2054] The server requested authentication method unknown to the client - 有欲 - 博客园</div></div></div></div><div class="relative w-100 w-30-l mb4 bg-white"><div class="relative w-100 mb4 bg-white nested-copy-line-height"><div class="bg-white mb3 pa4 gray overflow-hidden"><span class="f6 db">Posts</span><h1 class="f3 near-black"><a href=/posts/2021-03-04-%E8%AE%B0%E5%BD%95%E5%AE%89%E8%A3%85cuda/ class="link black dim"></a></h1><div class="nested-links f5 lh-copy nested-copy-line-height">记录安装cuda 获取下载命令： 进入官网
https://developer.nvidia.com/zh-cn/cuda-downloads
依次选择：Linux、x86、Ubuntu、20.04、runfile
获取下载命令：
wget https://developer.download.nvidia.com/compute/cuda/11.1.0/local_installers/cuda_11.1.0_455.23.05_linux.run sudo sh cuda_11.1.0_455.23.05_linux.run 安装 由于已经安装了显卡驱动，因此在安装时将驱动取消打勾。
安装完成后，显示提示。
=========== = Summary = =========== Driver: Not Selected Toolkit: Installed in /usr/local/cuda-11.1/ Samples: Installed in /home/yunze/, but missing recommended libraries Please make sure that - PATH includes /usr/local/cuda-11.1/bin - LD_LIBRARY_PATH includes /usr/local/cuda-11.1/lib64, or, add /usr/local/cuda-11.1/lib64 to /etc/ld.so.conf and run ldconfig as root To uninstall the CUDA Toolkit, run cuda-uninstaller in /usr/local/cuda-11.1/bin ***WARNING: Incomplete installation! This installation did not install the CUDA Driver.</div></div></div></div></section><ul class="pagination pagination-default"><li class=page-item><a href=/posts/ aria-label=First class=page-link role=button><span aria-hidden=true>&#171;&#171;</span></a></li><li class=page-item><a href=/posts/ aria-label=Previous class=page-link role=button><span aria-hidden=true>&#171;</span></a></li><li class=page-item><a href=/posts/ aria-label="Page 1" class=page-link role=button>1</a></li><li class="page-item active"><a aria-current=page aria-label="Page 2" class=page-link role=button>2</a></li><li class=page-item><a href=/posts/page/3/ aria-label="Page 3" class=page-link role=button>3</a></li><li class=page-item><a href=/posts/page/3/ aria-label=Next class=page-link role=button><span aria-hidden=true>&#187;</span></a></li><li class=page-item><a href=/posts/page/3/ aria-label=Last class=page-link role=button><span aria-hidden=true>&#187;&#187;</span></a></li></ul></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://arthuryunze.github.io/>&copy; Yunze's blog 2022</a><div><div class=ananke-socials></div></div></div></footer></body></html>
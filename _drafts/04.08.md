# CNN相关 

**滤波器（filter，也称为kernel）**

**1.padding 填白**

从上面的引子中，我们可以知道，原图像在经过filter卷积之后，变小了，从(8,8)变成了(6,6)。假设我们再卷一次，那大小就变成了(4,4)了。

**这样有啥问题呢？** 主要有两个问题： - 每次卷积，图像都缩小，这样卷不了几次就没了； - 相比于图片中间的点，图片边缘的点在卷积中被计算的次数很少。这样的话，边缘的信息就易于丢失。

为了解决这个问题，我们可以采用padding的方法。我们每次卷积前，先给图片周围都补一圈空白，让卷积之后图片跟原来一样大，同时，原来的边缘也被计算了更多次。



**2.stride 步长**

前面我们所介绍的卷积，都是默认步长是1，但实际上，我们可以设置步长为其他的值。 比如，对于(8,8)的输入，我们用(3,3)的filter， 如果stride=1，则输出为(6,6); 如果stride=2，则输出为(3,3);（这里例子举得不大好，除不断就向下取整）



**3.pooling 池化**

这个pooling，是为了提取一定区域的主要特征，并减少参数数量，防止模型过拟合。 比如下面的MaxPooling，采用了一个2×2的窗口，并取stride=2：



**4.对多通道（channels）图片的卷积**

这个需要单独提一下。彩色图像，一般都是RGB三个通道（channel）的，因此输入数据的维度一般有三个：**（长，宽，通道）**。 比如一个28×28的RGB图片，维度就是(28,28,3)。

前面的引子中，输入图片是2维的(8,8)，filter是(3,3)，输出也是2维的(6,6)。

如果输入图片是三维的呢（即增多了一个channels），比如是(8,8,3)，这个时候，我们的filter的维度就要变成(3,3,3)了，它的 **最后一维要跟输入的channel维度一致。** 这个时候的卷积，**是三个channel的所有元素对应相乘后求和**，也就是之前是9个乘积的和，现在是27个乘积的和。因此，输出的维度并不会变化。还是(6,6)。

但是，一般情况下，我们会 **使用多了filters同时卷积**，比如，如果我们同时使用4个filter的话，那么 **输出的维度则会变为(6,6,4)**。



## CNN的结构组成

**1. Convolutional layer（卷积层--CONV）**

由滤波器filters和激活函数构成。 一般要设置的超参数包括filters的数量、大小、步长，以及padding是“valid”还是“same”。当然，还包括选择什么激活函数。



**2. Pooling layer （池化层--POOL）**

这里里面没有参数需要我们学习，因为这里里面的参数都是我们设置好了，要么是Maxpooling，要么是Averagepooling。 需要指定的超参数，包括是Max还是average，窗口大小以及步长。 通常，我们使用的比较多的是Maxpooling,而且一般取大小为(2,2)步长为2的filter，这样，经过pooling之后，输入的长宽都会缩小2倍，channels不变。



**3. Fully Connected layer（全连接层--FC）**

这个前面没有讲，是因为这个就是我们最熟悉的家伙，**就是我们之前学的神经网络中的那种最普通的层，就是一排神经元**。因为这一层是每一个单元都和前一层的每一个单元相连接，所以称之为“全连接”。 这里要指定的超参数，无非就是神经元的数量，以及激活函数。



## 卷积神经网络相对于传统神经网络的优势

**1.参数共享机制（parameters sharing）**

**2.连接的稀疏性（sparsity of connections）**



---

reference:

[【DL笔记6】从此明白了卷积神经网络（CNN） - 知乎](https://zhuanlan.zhihu.com/p/42559190)